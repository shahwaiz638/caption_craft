{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "602737da-5bcb-4738-a4e3-326655e273d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CustomDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, data, seq_length, tokenizer):\n",
    "#         self.data = data\n",
    "#         self.seq_length = seq_length\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "# def load_data(file_path):\n",
    "#     # Initialize an empty list to store caption pairs\n",
    "#     caption_pairs = []\n",
    "\n",
    "#     # Read lines from the file\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         lines = file.readlines()\n",
    "\n",
    "#     # Iterate through lines to extract short and long captions\n",
    "#     i = 0\n",
    "#     while i < len(lines):\n",
    "#         if lines[i].startswith('short caption:'):\n",
    "#             short_caption = \"prompt: \"+lines[i].replace('short caption:', '').strip()\n",
    "#             i += 1\n",
    "#             if i < len(lines) and lines[i].startswith('long caption:'):\n",
    "#                 long_caption = \"response: \"+lines[i].replace('long caption:', '').strip()\n",
    "#                 caption_pairs.append((short_caption, long_caption))\n",
    "#             else:\n",
    "#                 # If the next line doesn't start with 'long caption:', skip the entry\n",
    "#                 i += 1\n",
    "#         else:\n",
    "#             i += 1\n",
    "\n",
    "#     return caption_pairs\n",
    "\n",
    "\n",
    "#     def __len__(self):\n",
    "#         if self.random_chunk:\n",
    "#             return len(self.data) // self.seq_length - 1\n",
    "#         else:\n",
    "#             return (len(self.data) - self.offset) // self.seq_length\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # Implement your logic to process each data point\n",
    "#         # This might include tokenization, padding, and returning a dictionary with input and label\n",
    "\n",
    "#         short_caption, long_caption = self.data[index]\n",
    "\n",
    "#         encoding = self.tokenizer(\n",
    "#             short_caption,\n",
    "#             long_caption,\n",
    "#             return_tensors=\"pt\",\n",
    "#             max_length=self.seq_length,\n",
    "#             padding=\"max_length\",\n",
    "#             truncation=True,\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "#             \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e4ae6d-acfa-4f32-bbd2-18bf987ceb13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama_finetune/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    GPT2TokenizerFast,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    LlamaTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import  Subset\n",
    "from random import sample\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "from distillation_arguments import DistillationTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "676e0cad-6987-4f3f-8a17-a6f5273a2f43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############\n",
    "LR = 2.5e-4\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LENGTH = 128\n",
    "\n",
    "TEMPERATURE = 2.0\n",
    "ALPHA = 0.5\n",
    "#############\n",
    "\n",
    "\n",
    "PATH = Path(\"./\")\n",
    "tokenizer_path = PATH / \"models/gpt-clean-32000.json\"\n",
    "tokenizer = GPT2TokenizerFast(tokenizer_file= str(tokenizer_path))\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "\n",
    "# teacher_dir1 = PATH / 'models/Llama-360M'\n",
    "# teacher_dir2 = PATH / 'models/gpt2'\n",
    "\n",
    "\n",
    "MODEL_NAME = f'Baby-Llama2-144M'\n",
    "MODEL_OUTPUT = Path('./models') /  MODEL_NAME\n",
    "EVAL_SAMPLES = 8192\n",
    "\n",
    "\n",
    "wandb_log = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc3436c-a9c6-4fbd-a5a1-cec64512225a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.93s/it]\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "teacher=None\n",
    "teacher1 = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",ignore_mismatched_sizes=True,load_in_4bit=True,device_map=\"auto\")\n",
    "#teacher2 = GPT2LMHeadModel.from_pretrained(teacher_dir2,ignore_mismatched_sizes=True)\n",
    "#teachers = [teacher1, teacher2]\n",
    "teachers = [teacher1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac8c79a1-bd9d-4afc-a669-ca7b73f5fd8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aeb4a0d-d9a8-4a34-b374-4cb3e81a6794",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='meta-llama/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9be42ad-c090-4547-a2e7-27140d31d89f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': False} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': False} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• data/captions_clean/captions.jsonl len: 9421509\n",
      "üî• data/captions_clean/.ipynb_checkpoints/captions-checkpoint.jsonl len: 9421509\n",
      "Saving data to data/captions_clean/tokenized_LlamaTokenizer_32000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': False} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• data/captions_dev_clean/captions.jsonl len: 2350986\n",
      "Saving data to data/captions_dev_clean/tokenized_LlamaTokenizer_32000.pt\n",
      "2350986\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from babylm_dataset import BabylmDataset\n",
    "tokenizer.model_max_length = SEQ_LENGTH\n",
    "print(tokenizer.model_max_length)\n",
    "train_dataset = BabylmDataset(\"data/captions_clean\", SEQ_LENGTH, tokenizer=tokenizer, random_chunk=True)\n",
    "full_eval_dataset = BabylmDataset( \"data/captions_dev_clean\", SEQ_LENGTH, tokenizer=tokenizer, offset=0)\n",
    "# eval_indices = sample(range(len(full_eval_dataset)), EVAL_SAMPLES)\n",
    "# eval_dataset = Subset(full_eval_dataset, eval_indices)\n",
    "\n",
    "eval_indices = sample(range(len(full_eval_dataset)), min(EVAL_SAMPLES, len(full_eval_dataset)))\n",
    "eval_dataset = Subset(full_eval_dataset, eval_indices)\n",
    "\n",
    "\n",
    "print(len(full_eval_dataset.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78f97210-689b-4033-b3bd-12624bbbf88a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2350986\n"
     ]
    }
   ],
   "source": [
    "print(len(full_eval_dataset.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fdd0bba-62b1-4f10-8023-a748a50c6263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=32000,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=20,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=8,\n",
    "    bos_token_id=tokenizer.convert_tokens_to_ids(\"<s>\"),\n",
    "    eos_token_id=tokenizer.convert_tokens_to_ids(\"</s>\"),\n",
    "    pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
    "    max_position_embeddings=2*SEQ_LENGTH,\n",
    ")\n",
    "\n",
    "student = LlamaForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00054753-6f34-4903-8c10-890f734a4591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': False} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [1, 910, 338, 365, 29880, 3304]\n",
      "Decoded String: <s> This is Llama\n",
      "Tokens: ['<s>', '‚ñÅThis', '‚ñÅis', '‚ñÅL', 'l', 'ama']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"This is Llama\"\n",
    "\n",
    "encoded = tokenizer.encode(text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"Encoded IDs: {encoded}\")\n",
    "print(f\"Decoded String: {decoded}\")\n",
    "\n",
    "# Access tokens using convert_ids_to_tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded)\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac7330b3-7dbd-4633-9ccc-fb75690ae19f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model num parameters: student  = 143555328\n",
      "model num parameters: teacher1 = 6738415616\n"
     ]
    }
   ],
   "source": [
    "teachers = [teacher1]\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(f'model num parameters: student  = {student.num_parameters()}')\n",
    "print(f'model num parameters: teacher1 = {teacher1.num_parameters()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e2da2eb-0ee6-474e-b9a8-21ea033cfa3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_models=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teachers = teacher_models\n",
    "        for teacher in self.teachers:\n",
    "            # place each teacher on same device as student\n",
    "            #self._move_model_to_device(teacher, self.model.device)\n",
    "            teacher.eval()\n",
    "        \n",
    "        \n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # compute student output\n",
    "        outputs_student = model(**inputs)\n",
    "        student_loss = outputs_student.loss\n",
    "\n",
    "        # compute teacher output\n",
    "        with torch.no_grad():\n",
    "            all_teacher_logits = []\n",
    "            for teacher in self.teachers:\n",
    "                outputs_teacher = teacher(**inputs)\n",
    "                all_teacher_logits.append(outputs_teacher.logits)\n",
    "            avg_teacher_logits = torch.stack(all_teacher_logits).mean(dim=0)\n",
    "\n",
    "        # assert size\n",
    "        # print(\"Student logits size:\", outputs_student.logits.size())\n",
    "        # print(\"Average teacher logits size:\", avg_teacher_logits.size())\n",
    "\n",
    "        #assert outputs_student.logits.size() == avg_teacher_logits.size()\n",
    "\n",
    "        # Soften probabilities and compute distillation loss\n",
    "        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_logits = (\n",
    "            loss_function(\n",
    "                F.log_softmax(outputs_student.logits / self.args.temperature, dim=-1),\n",
    "                F.softmax(avg_teacher_logits / self.args.temperature, dim=-1),\n",
    "            )\n",
    "            * (self.args.temperature ** 2)\n",
    "        )\n",
    "        # Return weighted student loss\n",
    "        loss = self.args.alpha * student_loss + (1.0 - self.args.alpha) * loss_logits\n",
    "        return (loss, outputs_student) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6474cc2-3c6f-4944-8238-27f93edf2e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshahwaiz638\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/Distillation/wandb/run-20240304_094423-br8o3o4s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shahwaiz638/distilled_llama2/runs/br8o3o4s' target=\"_blank\">Baby-Llama2-74M</a></strong> to <a href='https://wandb.ai/shahwaiz638/distilled_llama2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shahwaiz638/distilled_llama2' target=\"_blank\">https://wandb.ai/shahwaiz638/distilled_llama2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shahwaiz638/distilled_llama2/runs/br8o3o4s' target=\"_blank\">https://wandb.ai/shahwaiz638/distilled_llama2/runs/br8o3o4s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/shahwaiz638/distilled_llama2/runs/br8o3o4s?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4380d294d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#if wandb_log:\n",
    "wandb.login()\n",
    "wandb.init(project='distilled_llama2', name=MODEL_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bd23e8d-6eed-4f0f-afcb-2b4a0cc5943b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = DistillationTrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=6,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    save_total_limit=1,  # Set to zero to avoid saving\n",
    "    report_to=\"wandb\",\n",
    "    warmup_steps=200, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=LR,\n",
    "    logging_steps=20,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    weight_decay=0.1,\n",
    "    alpha=ALPHA,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_steps=1000,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = DistillationTrainer(\n",
    "        student,\n",
    "        training_args,\n",
    "        teacher_models=teachers,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41844819-2cbb-4870-8c57-47ee55b1d636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama_finetune/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "trainer.save_model(MODEL_OUTPUT)\n",
    "tokenizer.save_pretrained(MODEL_OUTPUT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4b0dfcd-55f4-4368-8345-95b37551eb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d675e8-bec0-42c0-baf6-5355f2ffcad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "llama_finetune",
   "name": ".m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m116"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
